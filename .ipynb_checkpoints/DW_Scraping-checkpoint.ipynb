{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "<!-- requirement: images/async.png -->\n",
    "<!-- requirement: small_data/Stanford-Tech-Listing.html -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll talk about \"scraping\": how to get unstructured data and turn it into something usable. We'll primarily focus on _web scraping_. Python has mature tools that make this pretty easy.\n",
    "\n",
    "The basic workflow is:\n",
    "\n",
    "1. Find the data you want on the web.\n",
    "2. Inspect the webpage and figure out how to select the content you want. This usually involves some combination of\n",
    "    - Viewing the source code of the page (especially if it is simple), and\n",
    "    - Figuring out the structure of the HTML parse tree.  This step is much easier with something like __Chrome Developer Tools__.\n",
    "3.  Write code to get out what you want:\n",
    "    - If the page is very simple, treat it as a bunch of text => __string manipulation / [regular expressions](https://docs.python.org/2/howto/regex.html)__ in Python.\n",
    "    - If the page is more complicated (and/or written in good style), we want to use the HTML parse tree => __[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) / [lxml](http://lxml.de/lxmlhtml.html)__ in Python.\n",
    "4.  Make sure it worked!\n",
    "5.  If your crawling problem is at all non-trivial, you will now have to go back to Step 2 to zoom in further -- or you'll have parsed the URL of a link you want to follow, in which case you'll go back to Step 1 to figure out how to parse what you want from the new target page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, suppose we want to crawl the list of \"Available Technologies\" being licensed by MIT at http://tlo.mit.edu/explore-mit-technologies/view-technologies and store their basic info, their associated patents, and the reference counts on their associated patents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding URLs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find the correct URL to use.\n",
    "\n",
    "- _First try_:  Aha, a list categories at the bottom.  Let's click on a few -- what do we see?  Many are empty, the categories are not obviously mutually exclusive, okay.  Maybe there's a better way.\n",
    "- _Second try_: Let's just click the search button on http://tlo.mit.edu/explore-mit-technologies/view-technologies.  Okay, better but it only gives us 10 at a time.  Are we going to have to click through all of the following pages?  Let's just click on page 2 to see what happens.\n",
    "- _Third try_: Aha, the URL for page 2 is http://tlo.mit.edu/technologies?search_api_views_fulltext=&page=1.  We can just advance the page number programmatically to visit all of the pages.\n",
    "\n",
    "> Sometimes you can play with the query string to change other options.  In the past, we were able to set `limit=1000` to get all of the technologies listed on one page.  This no longer works, but there could be an equivalent parameter we haven't noticed.  In the end, you are limited by what the web server will provide.\n",
    "\n",
    "For now, we'll just worry about getting the technologies off of the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://tlo.mit.edu/technologies\"\n",
    "response = requests.get(url, params={\"search_api_views_fulltext\": \"\"})\n",
    "print response.url\n",
    "response.text[:1000] + \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML and the DOM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started:\n",
    "\n",
    "- Pull up http://tlo.mit.edu/technologies?search_api_views_fulltext= in Chrome.  \n",
    "- Open __View->Developer->Developer Tools__.  \n",
    "- Right click on one of the technology titles, and choose __\"Inspect Element\"__.\n",
    "\n",
    "What are we looking at?  Well... this is the structure of the webpage.  Nested _tags_ of different _types_ and having a variety of _attributes_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we learned above:\n",
    "\n",
    "  - All of the technologies are underneath (\"_descendents of_\")   `<section id=\"block-system-main\">`\n",
    "  - In fact, each of them is in its own `<div class=\"views-row\">`\n",
    "  \n",
    "Now we're ready to move on:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to parse the raw HTML and actually grab the links of detailed info. The two main parser libraries in Python are `BeautifulSoup` and `lxml`. `lxml` is much faster (it leverages several C libraries), but it's also worse at dealing with malformed, crummy HTML. Because parsing speed isn't our bottleneck here, we'll use `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.text)\n",
    "print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parent = soup.find('section', attrs={'id': 'block-system-main'}) #Find (at most) *one*\n",
    "parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_divs = parent.find_all('div', attrs={'class':'views-row'})  #Find *all*\n",
    "len(tech_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS selectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern of nested finds, based on tag type, ID, and class, is very common. It's so common that there are two special convenience languages for such traversals: [CSS selectors](http://www.w3schools.com/cssref/css_selectors.asp) and [XPath](http://www.w3schools.com/xml/xpath_intro.asp) (which works for all XML, not just HTML). We'll be using CSS selectors, which are more common for HTML and easier to learn.\n",
    "\n",
    "With CSS selectors, we can write the above in a more concise and expressive way:\n",
    "\n",
    "```python\n",
    "tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "```\n",
    "\n",
    "All selectors work like 'find_all'.  Some basic building examples of selectors are:\n",
    "\n",
    " - `'mytag'` picks out all tags of type `mytag`.\n",
    " - `'#myid'` picks out all tags whose _id_ is equal to `myid`\n",
    " - `'.myclass'` picks out all tags whose _class_ is equal to `myclass`\n",
    " - `'mytag#myid'` will pick all tags of type `mytag` **and** `id` equal to `myid` (analogously for `'mytag.myclass'`)\n",
    " - If `'selector1'` and `'selector2'` are two selectors, then there is another selector `'selector1 selector2'`.  It picks out all tags satisfying `selector2` that are __descendents__(*) of something satisfying `selector1`, i.e. it's like our nested find.\n",
    " \n",
    " (*) It doesn't have to be a _direct_ descendent.  I.e. it can be a grand-grand-...-grand-child of something satisfying `selector1`.  For direct descendents we'd instead write `'selector1 > selector2'`\n",
    " \n",
    "Let's just see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print soup.select('section#block-system-main')[0].prettify()[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print soup.select('section#block-system-main div.views-row')[0].prettify()[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_divs = soup.select('section#block-system-main div.views-row')\n",
    "len(tech_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to pull out some key pieces of info:\n",
    "\n",
    "- The technology's \"title\" (the text in the `<a>` element)\n",
    "- The link to follow for more info on the technology (the _href_ attribute of the `<a>`)\n",
    "- And a short blurb about the text (in the following `<span>`)\n",
    "\n",
    "Let's write some code to extract this.  But before we do, let's discuss what _form_ the output should take: It is often convenient to store data in a dictionary (i.e. as a _key-value_ hashtable) - in other words, to name the bits of data you are collecting.  One big advantage is that this makes it easier to add in extra fields progressively.\n",
    "\n",
    "Let's see what the code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "firsta = tech_divs[0].select('a')[0]\n",
    "firsta.text, firsta['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a \"named tuple\" to store our key-value data.\n",
    "We could also have used a dictionary, with strings as keys.\n",
    "Named tuples have some advantages:\n",
    " - Better notation with autocomplete, x.field_name instead of x['field_name']\n",
    " - If you change your object structure later and fail to update your\n",
    "   code to include the new fields, this will make it easier to find.\n",
    " - They are immutable, preventing certain sorts of bugs.\n",
    "\n",
    "... and some disadvantages:\n",
    " - If you want to augment object structure you need a new type\n",
    "   (or to go back and fill your code)\n",
    " - They are immutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "TechBasic = namedtuple('TechBasic', 'title, url, short')\n",
    "\n",
    "def td_info(td):\n",
    "    la = td.select('a')\n",
    "    ls = td.select('span')\n",
    "    if len(la) != 1 or len(ls) != 2:\n",
    "        print \"Uh oh! We did something wrong for:\"\n",
    "        print \"\\n\".join(\">>> \" + line for line in td.prettify().split(\"\\n\"))\n",
    "        return\n",
    "    return TechBasic(title=la[0].text, url=la[0]['href'], short=ls[1].text)\n",
    "\n",
    "tech_links = filter(None, [td_info(td) for td in tech_divs])\n",
    "\n",
    "tech_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching subsequent pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can get all of the information off of one page, let's figure out how to grab the data from all the subsequent pages, too.  As a first step, let's encapsulate the parsing into function.  For reasons that will become apparent soon, we take a HTTP response as an argument, instead of doing the request in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_techs(response):\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    tech_divs = soup.select('section#block-system-main div.views-row')\n",
    "    return filter(None, [td_info(td) for td in tech_divs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we'll only get data from the first 10 pages.  Since we'll be using them often, we'll write a function to give us the arguments to `requests.get` to obtain the $n^{th}$ page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LIMIT = 10\n",
    "def get_page_args(i):\n",
    "    return {\"url\": url,\n",
    "            \"params\": {\"search_api_views_fulltext\": \"\", \"page\": i}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 1:** The first solution is to run the requests serially.  This is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "# Slow version\n",
    "\n",
    "techs = [get_techs(requests.get(**get_page_args(i))) for i in xrange(LIMIT)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that connecting to a remote server and fetching the pages takes a while. Scraping web pages is usually _IO-bound_ and not CPU-bound (that is, we spent most of our time waiting for data and not processing it). Fortunately, Python gives us lots of different ways to deal with this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 2:** We can use Python's [multiprocessing](https://docs.python.org/2/library/multiprocessing.html) interface, which can easily parallelize a map.  This is a very straightforward API to use.  The drawback of this is that it spins up independent processes, which have a potentially significant download time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define this function in a separate cell due to a problematic\n",
    "# interaction between timeit and multiprocessing.\n",
    "def get_page(i):\n",
    "    return requests.get(**get_page_args(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "# faster version -- using multiprocessing\n",
    "\n",
    "from multiprocessing import Pool\n",
    "p = Pool(3)\n",
    "\n",
    "#def get_page(i):\n",
    "#    return requests.get(url, {\"search_api_views_fulltext\": \"\", \"page\": i})\n",
    "\n",
    "responses = p.map(get_page, xrange(LIMIT))\n",
    "techs = [get_techs(response) for response in responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution 3:** For requests, there is a special library called [requests-futures](https://github.com/ross/requests-futures) which returns a placeholder object that holds a promise to return the webpage sometime later (in the \"future\").  This allows us to continue making other fetching requests while waiting for the first result to return.\n",
    "\n",
    "![Synchronous vs. Asynchronous](images/async.png)\n",
    "\n",
    "Requests-futures works by combining the `requests` library with `concurrent.futures`.  For a faster, though harder to debug, alternative, you can look at [grequests](https://github.com/kennethreitz/grequests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "# faster version using requests-futures\n",
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "session = FuturesSession(max_workers=5)\n",
    "futures = [session.get(**get_page_args(i)) for i in xrange(LIMIT)]\n",
    "\n",
    "techs = [get_techs(future.result()) for future in futures]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'll take a little bit, but now we can scrape all of the listed technologies.  To avoid getting nested lists, we use a double comprehension, which flattens the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "session = FuturesSession(max_workers=5)\n",
    "techs = [tech \n",
    "         for future in [session.get(**get_page_args(i)) for i in xrange(61)]\n",
    "         for tech in get_techs(future.result())]\n",
    "len(techs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "techs[150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "\n",
    "1. Write a function `get_tech_details` to parse the individual page for each technology.  It can capture the longer description, the inventors, patent information, etc.\n",
    "\n",
    "2. Use `requests_futures` to get all of the individual pages.  Parse them with your `get_tech_details` function to produce a more detailed database of MIT inventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are really interested in crawling, consider using `scrapy`.  [Scrapy](http://scrapy.org/) is a specialized python package for scraping websites.  In particular, it has a few features:\n",
    "1. The HTML is parsed and accessed through a `response` object in a `parse` method which in turn supports `response.xpath` and `response.css` methods, allowing one to use `xpath` and `css` selectors on the response dom, respectively.\n",
    "1. Data is stored in `scrapy.Item` objects (which are similar to `namedtuple`s) or as python dictionaries.\n",
    "1. Scrapy is object-oriented and calls it's own `parse` method (a generator) that `yield`s values.\n",
    "1. You can limit your crawls through specifying the class property `allowed_domains` and definte the starting point of your crawl using the class property `start_urls`.\n",
    "1. You can also build pipelines of crawling and data extraction steps to make sure crawling code more usable.\n",
    "1. Additional scraping steps (e.g. scraping entries in a directory like in the example above) can be accessed via `scrapy.Request`.\n",
    "1. It has command lines arguements to allow you to interactively play with the the `response` object from a webpage (`scrapy shell`) or view a page as the library renders it, which may be different from how your browser renders it (`scrapy view`).\n",
    "\n",
    "The following is a canonical `scrapy` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class StackOverflowSpider(scrapy.Spider):\n",
    "    name = 'stackoverflow'\n",
    "    start_urls = ['http://stackoverflow.com/questions?sort=votes']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for href in response.css('.question-summary h3 a::attr(href)'):\n",
    "            full_url = response.urljoin(href.extract())\n",
    "            yield scrapy.Request(full_url, callback=self.parse_question)\n",
    "\n",
    "    def parse_question(self, response):\n",
    "        yield {\n",
    "            'title': response.css('h1 a::text').extract()[0],\n",
    "            'votes': response.css('.question .vote-count-post::text').extract()[0],\n",
    "            'body': response.css('.question .post-text').extract()[0],\n",
    "            'tags': response.css('.question .post-tag::text').extract(),\n",
    "            'link': response.url,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More complicated example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we had picked Stanford instead of MIT.  Let's try to do the same thing (it's a bit harder to get a good listing URL, so I just downloaded one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from urlparse import urljoin\n",
    "\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import requests\n",
    "\n",
    "with open(\"small_data/Stanford-Tech-Listing.html\", \"r\") as fin:\n",
    "    soup = BeautifulSoup(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BeautifulSoup doesn't seem to support 'or' selectors, so:\n",
    "selector = lambda x: x.has_attr(\"id\") and x[\"id\"].startswith(\"output_row\")\n",
    "tech_rows = soup.find_all(selector)[1:]\n",
    "# Alternate -- showing how to go up and down the tree\n",
    "#tech_rows = soup.find('tr', attrs={'id':'output_row_1'}).parent.findAll('tr')[1:]\n",
    "print len(tech_rows)\n",
    "print tech_rows[0].prettify()\n",
    "print tech_rows[-1].prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Details:** Let's quickly break down that last line for two bits of Python syntax that we haven't explicitly talked about\n",
    "    >    selector = lambda x: x.has_attr('id') and x['id'].startswith('output_row')\n",
    "This is a \"lambda expresion\" -- a short, inline, unnamed function. Lambdas are pretty limited, so you should define a named function for anything complicated  \n",
    "\n",
    "    >    tech_rows = soup.find_all(selector)[1:]      \n",
    "                                            ^^^^\n",
    "This is list slice notation (we already used this above with [:2]!).  In this case, we're taking all but the zero-th entry (which is a list header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UH OH:** \n",
    "When originally preparing this, I was using Anaconda.  The same code only showed about _254_ of the _1727_ entries -- BeautifulSoup was incorrectly parsing the file.  These sorts of things are not entirely uncommon, so sometimes it helps to double-check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Warning: This is hacky code!\n",
    "TechBlurb = namedtuple('TechBlurb', 'docket techid url title')\n",
    "def parse_tr(tr):\n",
    "    link = tr.select(\"td.output_data a\")[0]\n",
    "    \n",
    "    docket = link.text\n",
    "    url = link[\"href\"]\n",
    "    techid = url.split(\"=\")[1]\n",
    "    title = tr.select(\"td.output_data\")[2].text\n",
    "    return TechBlurb(docket=docket, techid=techid, url=url, title=title)\n",
    "tech_blurbs=map(parse_tr, tech_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And this isn't much better!\n",
    "def find_comment_by_text_in(soup, comment_text):\n",
    "    return soup.find(text=lambda text: isinstance(text, Comment) and comment_text in text)\n",
    "\n",
    "TechDetailed = namedtuple('TechDetailed', 'blurb, abstract, similar')\n",
    "SimilarHint = namedtuple('SimilarHint', 'techid, docket, title')\n",
    "def get_tech_details(response):\n",
    "    # We're doing a lot of chaining with implicit assumptions here -- \n",
    "    #   it might fail in all sorts of way, in which case we give up.\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    contents = soup.find_all('form')[1]\n",
    "    abstract = (find_comment_by_text_in(contents, 'Abstract')\n",
    "        .find_next_sibling('hr')\n",
    "        .find('div')\n",
    "        .text)\n",
    "        \n",
    "    def parse_similar_tr(r):\n",
    "        tds = r.find_all('td')\n",
    "        if len(tds) < 3:\n",
    "            return None\n",
    "        return SimilarHint (\n",
    "            techid = tds[0].find('a')['href'].split('=')[1], \n",
    "            docket = \"S\"+tds[0].text.strip(), \n",
    "            title  = tds[2].text.strip()\n",
    "        )\n",
    "\n",
    "    similar_trs = (find_comment_by_text_in(soup.find_all('form')[1], 'Similar Tech')\n",
    "                      .find_next_sibling('table')\n",
    "                      .find('div')\n",
    "                      .find('table')\n",
    "                      .find('table')\n",
    "                      .find_all('tr'))\n",
    "    similar = filter(None, [parse_similar_tr(tr) for tr in similar_trs])\n",
    "    \n",
    "    return TechDetailed(blurb=blurb, abstract=abstract, similar=similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Since the point is to show that something goes wrong, let's not wait until the end!\n",
    "# imap_unordered lets you use the results of the map as they are produced (rather than storing them)\n",
    "# and with no guarantee on order.\n",
    "\n",
    "url_base=\"http://techfinder.stanford.edu/\"\n",
    "\n",
    "for blurb in tech_blurbs:\n",
    "    response = requests.get(urljoin(url_base, blurb.url))\n",
    "    try:\n",
    "        get_tech_details(response)\n",
    "    except:\n",
    "        print \"Something went wrong!\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the above code, it tells us that [this technology](http://techfinder.stanford.edu/technology_detail.php?ID=30261) did not have a list of similar technologies.  But going to the web page shows that it does!  What went wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://techfinder.stanford.edu/technology_detail.php'\n",
    "soup = BeautifulSoup(requests.get(url, params={\"ID\": 30261}).text)\n",
    "contents = soup.find_all('form')[1]\n",
    "print contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go and look at the same part of the **raw** HTML, we find that there is no `</form>` there:\n",
    "\n",
    "    >    <!--- Applications --->\n",
    "    >    <h3>Applications</h3><br/>\n",
    "    >    <ul><li>Imaging apoptosis<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Research</li><li>Clinical<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Monitor therapeutic efficacy in cancer patients</li><li>Anti-cancer drug selection</ul></ul></li></ul><br/>\n",
    "    >    \n",
    "    >    <!--- Advantages --->\n",
    "    >    <h3>Advantages</h3><br/>\n",
    "    >    <ul><li>High specificity for caspase-3 and -7</li><li>High sensitivity</li><li>Non-invasive</li><li>Biocompatible</li><li>Small size of probe allows:<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Deep tissue penetration</li><li>More extensive biodistribution</ul></li><li>PET probes:<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>High tumor/muscle ratio in apoptotic tumors</li><li>High uptake value in apoptotic tumors</ul></li><li>Fluorescent probe:<ul type=\"circle\" style=\"margin-bottom:0in\"></li><li>Possess NIR spectral properties</ul></li><li>May help promote personalized cancer medicine</li><li>Potential for probe design strategy to be applied to other enzyme targets</li></ul><br/>\n",
    "\n",
    "What there **is** is _mal-formed HTML_ that is bad enough to confuse BeautifulSoup.  (Note that it's not nearly bad enough to confuse a web browser however).  If you look at more examples, you will find even worse ones -- a stray `</html>` in the middle of a document is not unheard of.  \n",
    "\n",
    "To fix this, we can pre-\"tidy\" the page before feeding it to BeautifulSoup using **pytidylib**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tidylib import tidy_document\n",
    "url='http://techfinder.stanford.edu/technology_detail.php'\n",
    "\n",
    "tidy_page, __ = tidy_document(requests.get(url, params={\"ID\": 30261}).text)\n",
    "soup = BeautifulSoup(tidy_page)\n",
    "contents = soup.find_all('form')[1]\n",
    "print contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go back and modify `get_tech_details` to use this 'tidy' approach.\n",
    "\n",
    "2. Sometimes web servers are slow and/or unreliable, and sometimes your connection is.  If we were to run the above test twice, we'd probably find that some of the failures were just due to a connection error.  We didn't notice this because the _outer_ `try` / `except` is also catching these.  So: Modify `get_tech_details` to allow up to 3 retries. <br/>Bonus points if you actually look at what exceptions `urllib` throws in those cases instead of a general catch-all mechanism.  Alternate type of bonus points if you figure out how to do it using the `retrying` package.  You can test these by throttling your internet on and off to simulate an unreliable connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exit Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How would you design a web scraping app such that the user interface remained responsive? One that is robust to poor internet connections?\n",
    "1. How would you deal with messy/malformatted HTML/XML?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2015 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
